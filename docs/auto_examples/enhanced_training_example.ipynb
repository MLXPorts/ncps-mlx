{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example demonstrating enhanced training with different cell types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'CTRNN' from 'ncps' (/Users/sydneybach/miniconda3/lib/python3.12/site-packages/ncps/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mncps\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CTRNN, CTGRU, ELTC\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mncps\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EnhancedLTCTrainer, TrainingConfig\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Generate sequence data\u001b[39;00m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'CTRNN' from 'ncps' (/Users/sydneybach/miniconda3/lib/python3.12/site-packages/ncps/__init__.py)"
          ]
        }
      ],
      "source": [
        "import mlx.core as mx\n",
        "import mlx.nn as nn\n",
        "import mlx.optimizers as optim\n",
        "from ncps import CTRNN, CTGRU, ELTC\n",
        "from ncps.mlx.training import EnhancedLTCTrainer, TrainingConfig\n",
        "\n",
        "# Generate sequence data\n",
        "N = 1000  # sequence length\n",
        "in_features = 2  # input dimension\n",
        "out_features = 1  # output dimension\n",
        "batch_size = 1  # using batch_size=1 for this example\n",
        "\n",
        "# Generate input sequence: [batch_size, seq_length, features]\n",
        "data_x = mx.stack([\n",
        "    mx.sin(mx.linspace(0, 3 * mx.pi, N)), \n",
        "    mx.cos(mx.linspace(0, 3 * mx.pi, N))\n",
        "], axis=1)  # Shape: [N, 2]\n",
        "data_x = mx.expand_dims(data_x, axis=0)  # Shape: [1, N, 2]\n",
        "\n",
        "# Generate target sequence: [batch_size, seq_length, output_dim]\n",
        "data_y = mx.sin(mx.linspace(0, 6 * mx.pi, N))  # Shape: [N]\n",
        "data_y = mx.reshape(data_y, (1, N, out_features))  # Shape: [1, N, 1]\n",
        "\n",
        "# Generate time delta information: [batch_size, seq_length]\n",
        "# Use moderate time steps for stability\n",
        "time_delta = mx.clip(mx.random.uniform(low=0.05, high=0.15, shape=(batch_size, N)), 0.05, 0.15)\n",
        "\n",
        "# Initialize with explicit random seed for reproducibility\n",
        "mx.random.seed(42)\n",
        "\n",
        "# Training configuration with more conservative hyperparameters\n",
        "config = TrainingConfig(\n",
        "    target_accuracy=95.0,\n",
        "    max_epochs=2000,  # Increased max epochs for slower learning\n",
        "    patience=200,     # Increased patience\n",
        "    weight_clip=0.1,  # Reduced weight clip\n",
        "    bias_clip=0.05,   # Reduced bias clip\n",
        "    max_grad_norm=0.5,  # Reduced grad norm\n",
        "    learning_rate=0.001,  # Reduced learning rate\n",
        "    min_learning_rate=0.0001,  # Lower min learning rate\n",
        "    warmup_epochs=500,  # Longer warmup period\n",
        "    noise_scale=0.01,  # Reduced noise scale\n",
        "    noise_decay=0.9999,  # Slower noise decay\n",
        "    momentum=0.99,  # Higher momentum\n",
        "    grad_momentum=0.1  # Low gradient momentum\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = EnhancedLTCTrainer(config)\n",
        "\n",
        "# Define model configurations\n",
        "model_configs = [\n",
        "    (\n",
        "        \"CTRNN\",\n",
        "        CTRNN(\n",
        "            units=32,  # Moderate number of units\n",
        "            activation=\"tanh\",  # Using string activation name\n",
        "            cell_clip=0.5  # Moderate cell clip\n",
        "        )\n",
        "    ),\n",
        "    (\n",
        "        \"CTGRU\",\n",
        "        CTGRU(\n",
        "            units=32,  # Moderate number of units\n",
        "            cell_clip=0.5  # Moderate cell clip\n",
        "        )\n",
        "    ),\n",
        "    (\n",
        "        \"ELTC\",\n",
        "        ELTC(\n",
        "            input_size=in_features,\n",
        "            hidden_size=32,\n",
        "            ode_unfolds=6,\n",
        "            activation=\"tanh\",\n",
        "            cell_clip=0.5,  # Moderate cell clip\n",
        "            return_sequences=True\n",
        "        )\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train each model configuration\n",
        "for name, model in model_configs:\n",
        "    print(f\"\\nTraining {name}\")\n",
        "    \n",
        "    # Initialize model by doing a forward pass\n",
        "    dummy_input = mx.zeros((batch_size, 1, in_features))\n",
        "    _ = model(dummy_input)\n",
        "    \n",
        "    # Print model parameters\n",
        "    print(\"Model parameters:\")\n",
        "    def print_params(params, prefix=\"\"):\n",
        "        for name, param in params.items():\n",
        "            if isinstance(param, dict):\n",
        "                print(f\"{prefix}{name}:\")\n",
        "                print_params(param, prefix + \"  \")\n",
        "            elif isinstance(param, list):\n",
        "                print(f\"{prefix}{name}: list of {len(param)} items\")\n",
        "                for i, item in enumerate(param):\n",
        "                    if hasattr(item, 'shape'):\n",
        "                        print(f\"{prefix}  [{i}]: shape={item.shape}\")\n",
        "                    else:\n",
        "                        print(f\"{prefix}  [{i}]: {type(item)}\")\n",
        "            elif hasattr(param, 'shape'):\n",
        "                print(f\"{prefix}{name}: shape={param.shape}\")\n",
        "            else:\n",
        "                print(f\"{prefix}{name}: {type(param)}\")\n",
        "    \n",
        "    print_params(model.parameters())\n",
        "    \n",
        "    # Train model\n",
        "    history = trainer.train_with_accuracy_target(\n",
        "        model=model,\n",
        "        data_x=data_x,\n",
        "        data_y=data_y,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"Best accuracy: {history['best_accuracy']:.2f}%\")\n",
        "    print(f\"Best epoch: {history['best_epoch']}\")\n",
        "    print(f\"Converged: {history['converged']}\")\n",
        "    print(f\"Final loss: {history['loss'][-1]:.6f}\")\n",
        "    \n",
        "    # Evaluate final predictions\n",
        "    predictions = model(data_x, time_delta=time_delta)\n",
        "    final_loss = mx.mean((predictions - data_y) ** 2)\n",
        "    mx.eval(final_loss)  # Force evaluation\n",
        "    print(f\"Evaluation loss: {final_loss.item():.6f}\")\n",
        "\n",
        "print(\"\\nTraining complete.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
